import asyncio
from urllib.parse import urlparse, urljoin
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator

from database.db_setup import async_session, init_db
from database.models import Category, SiteSetting
from sqlalchemy import select

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫
translator = GoogleTranslator(source='auto', target='ru')

# –¢–û–ß–ù–´–ï –ê–î–†–ï–°–ê –†–ê–ó–î–ï–õ–û–í (–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å —Å–∞–π—Ç–æ–º)
TARGET_PAGES = {
    "–ï–ª–µ–∫—Ç—Ä–æ–Ω—ñ–∫–∞": "https://exp-shop.com/electronik.html",
    "–û–¥—è–≥ —Ç–∞ –≤–∑—É—Ç—Ç—è": "https://exp-shop.com/clothing.html",
    "–ö–æ—Å–º–µ—Ç–∏–∫–∞": "https://exp-shop.com/cosmetic.html",
    "–ì–æ–¥–∏–Ω–Ω–∏–∫–∏": "https://exp-shop.com/watch.html",
    "–ê–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏–Ω–∏": "https://exp-shop.com/autoparts.html",
    "–Ü–Ω—à–µ": "https://exp-shop.com/other.html"
}


async def translate_text(text: str) -> str:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫."""
    try:
        if not text or len(text) < 3:
            return text
        # deep-translator —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –∑–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        translated = await asyncio.to_thread(translator.translate, text)
        return translated if translated else text
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text


async def get_site_info(context, url):
    """–ó–∞—Ö–æ–¥–∏—Ç –Ω–∞ —Å–∞–π—Ç –º–∞–≥–∞–∑–∏–Ω–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è '–ø–∞—Å–ø–æ—Ä—Ç–∞' (–æ–ø–∏—Å–∞–Ω–∏—è)."""
    page = await context.new_page()
    try:
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç 15 —Å–µ–∫, —á—Ç–æ–±—ã –Ω–µ —Ç–æ—Ä–º–æ–∑–∏—Ç—å –æ–±—â—É—é –º–∏–≥—Ä–∞—Ü–∏—é
        await page.goto(url, wait_until="domcontentloaded", timeout=15000)

        # –ò—â–µ–º Meta Description
        description = await page.get_attribute('meta[name="description"]', "content")
        if not description:
            description = await page.title()

        if description:
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ —Ä—É—Å—Å–∫–∏–π –¥–ª—è '–±–ª–µ—Å–∫–∞ –∏ —à–∏–∫–∞'
            translated = await translate_text(description.strip())
            return translated[:180]

        return "–ü—Ä–µ–º–∏–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä —Ç–æ–≤–∞—Ä–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π –≤ –£–∫—Ä–∞–∏–Ω—É. –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –ª—É—á—à–∏–µ –±—Ä–µ–Ω–¥—ã."
    except Exception:
        return "–ú–∞–≥–∞–∑–∏–Ω –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∑–∞–∫–∞–∑–∞ —á–µ—Ä–µ–∑ –Ω–∞—à —Å–µ—Ä–≤–∏—Å. –ì–∞—Ä–∞–Ω—Ç–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞."
    finally:
        await page.close()


async def get_clean_name(link_tag, domain):
    """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞, —É–±–∏—Ä–∞—è –º—É—Å–æ—Ä —Ç–∏–ø–∞ 'Shop' –∏–ª–∏ 'T'."""
    img = link_tag.find('img')
    raw_name = ""
    if img:
        raw_name = img.get('alt') or img.get('title') or ""

    clean_name = raw_name.strip()

    # –ï—Å–ª–∏ –Ω–∞ —Å–∞–π—Ç–µ –Ω–µ—Ç –≤–Ω—è—Ç–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è, –¥–µ–ª–∞–µ–º –µ–≥–æ –∏–∑ –¥–æ–º–µ–Ω–∞
    if not clean_name or clean_name.lower() in ['shop', 'us', 'site', 'link', 't', 'index']:
        parts = domain.split('.')
        clean_name = parts[-2].capitalize() if len(parts) > 1 else parts[0].capitalize()

    return clean_name


async def scrape_and_migrate():
    print("üöÄ –ó–∞–ø—É—Å–∫ –≥–ª—É–±–æ–∫–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏ —Å –∞–≤—Ç–æ–ø–µ—Ä–µ–≤–æ–¥–æ–º (–°—Ç–∞–Ω–¥–∞—Ä—Ç: –¢–µ—Ö–∫–æ–Ω—Ç—Ä–æ–ª—å)...")
    await init_db()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        catalog_page = await context.new_page()

        async with async_session() as session:
            for cat_name, url in TARGET_PAGES.items():
                print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–¥–µ–ª–∞: {cat_name.upper()}")

                res = await session.execute(select(Category).where(Category.name == cat_name))
                category = res.scalar_one_or_none()
                if not category:
                    category = Category(name=cat_name);
                    session.add(category);
                    await session.flush()

                try:
                    await catalog_page.goto(url, wait_until="networkidle", timeout=60000)

                    # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤—Å–µ—Ö –ª–æ–≥–æ—Ç–∏–ø–æ–≤ (Lazy Loading)
                    for _ in range(8):
                        await catalog_page.mouse.wheel(0, 1500)
                        await asyncio.sleep(0.7)

                    soup = BeautifulSoup(await catalog_page.content(), 'html.parser')

                    shops_added = 0
                    for a in soup.find_all('a', href=True):
                        href = a['href']
                        if not href.startswith('http') or "exp-shop" in href:
                            continue

                        domain = urlparse(href).netloc.replace('www.', '')
                        if any(x in domain for x in ["facebook", "google", "instagram", "youtube", "t.me"]):
                            continue

                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                        check = await session.execute(select(SiteSetting).where(SiteSetting.domain == domain))
                        if check.scalar_one_or_none():
                            continue

                        shop_name = await get_clean_name(a, domain)
                        img = a.find('img')
                        logo = None
                        if img:
                            src = img.get('src') or img.get('data-src') or img.get('data-original')
                            if src:
                                logo = urljoin("https://exp-shop.com", src)

                        print(f"   üîé –ê–Ω–∞–ª–∏–∑ –∏ –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è: {shop_name}...")
                        description = await get_site_info(context, href)

                        new_shop = SiteSetting(
                            category_id=category.id,
                            name=shop_name,
                            domain=domain,
                            url=href,
                            logo_url=logo,
                            description=description
                        )
                        session.add(new_shop)
                        shops_added += 1

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—Ü–∏—è–º–∏
                        if shops_added % 5 == 0:
                            await session.commit()

                    print(f"üìà –ò—Ç–æ–≥ –ø–æ {cat_name}: +{shops_added} –º–∞–≥–∞–∑–∏–Ω–æ–≤")
                    await session.commit()

                except Exception as e:
                    print(f"‚ùå –°–±–æ–π –≤ {cat_name}: {e}")

        await browser.close()
    print("\n‚ú® –ú–∏–≥—Ä–∞—Ü–∏—è –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º —Ç–µ—Ö–∫–æ–Ω—Ç—Ä–æ–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    asyncio.run(scrape_and_migrate())